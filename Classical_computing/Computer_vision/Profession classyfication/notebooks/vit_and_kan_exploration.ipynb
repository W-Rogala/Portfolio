{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Models for Profession Classification\n",
    "\n",
    "This notebook explores the use of Vision Transformers (ViT) and KAN layers for the profession classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.download import download_and_extract_idenprof\n",
    "from src.data.preprocess import get_data_loaders, get_augmented_data_loaders, get_class_names\n",
    "from src.models.vit import vit_tiny, vit_small, vit_base\n",
    "from src.models.kan import add_kan_layer\n",
    "from src.utils.metrics import evaluate_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download and extract dataset\n",
    "data_dir = download_and_extract_idenprof(data_dir='../data')\n",
    "print(f\"Dataset directory: {data_dir}\")\n",
    "\n",
    "# Get class names\n",
    "class_names = get_class_names(data_dir)\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 32\n",
    "image_size = 224\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, test_loader = get_data_loaders(data_dir, batch_size, image_size)\n",
    "aug_train_loader, _ = get_augmented_data_loaders(\n",
    "    data_dir, \n",
    "    batch_size=batch_size, \n",
    "    image_size=image_size,\n",
    "    rotation=30,\n",
    "    hue=0.05,\n",
    "    saturation=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Vision Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create Vision Transformer models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "models = {\n",
    "    \"ViT-Tiny\": vit_tiny(num_classes=len(class_names)),\n",
    "    \"ViT-Small\": vit_small(num_classes=len(class_names)),\n",
    "    \"ViT-Base\": vit_base(num_classes=len(class_names))\n",
    "}\n",
    "\n",
    "# Display model architectures and parameter counts\n",
    "model_stats = []\n",
    "for name, model in models.items():\n",
    "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_stats.append({\n",
    "        \"Model\": name,\n",
    "        \"Parameters\": param_count,\n",
    "        \"Parameters (M)\": param_count / 1_000_000\n",
    "    })\n",
    "    \n",
    "# Create DataFrame and display\n",
    "df_stats = pd.DataFrame(model_stats)\n",
    "df_stats.set_index(\"Model\", inplace=True)\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get a sample batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "\n",
    "# Show first image\n",
    "img = images[0].permute(1, 2, 0) / 2 + 0.5  # Convert to HWC and denormalize\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Class: {class_names[labels[0].item()]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Explore ViT architecture\n",
    "vit = models[\"ViT-Tiny\"]\n",
    "\n",
    "# Examine patch embedding\n",
    "print(f\"Patch embedding layer: {vit.patch_embed}\")\n",
    "print(f\"Patch size: {vit.patch_embed.patch_size}\")\n",
    "print(f\"Number of patches: {vit.patch_embed.num_patches}\")\n",
    "\n",
    "# Examine attention mechanism\n",
    "print(f\"\\nFirst transformer block:\")\n",
    "print(f\"Attention layer: {vit.blocks[0].attn}\")\n",
    "print(f\"Number of attention heads: {vit.blocks[0].attn.num_heads}\")\n",
    "print(f\"MLP layer: {vit.blocks[0].mlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to visualize patches\n",
    "def visualize_patches(image, patch_size=16):\n",
    "    # Get image dimensions\n",
    "    _, h, w = image.shape\n",
    "    \n",
    "    # Calculate number of patches in each dimension\n",
    "    n_h = h // patch_size\n",
    "    n_w = w // patch_size\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_h, n_w, figsize=(10, 10))\n",
    "    \n",
    "    # Extract and display patches\n",
    "    for i in range(n_h):\n",
    "        for j in range(n_w):\n",
    "            # Extract patch\n",
    "            patch = image[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
    "            \n",
    "            # Convert to HWC and denormalize\n",
    "            patch = patch.permute(1, 2, 0) / 2 + 0.5\n",
    "            \n",
    "            # Display patch\n",
    "            axes[i, j].imshow(patch)\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Select an image\n",
    "sample_image = images[0]\n",
    "\n",
    "# Display original image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_image.permute(1, 2, 0) / 2 + 0.5)\n",
    "plt.title(f\"Original Image: {class_names[labels[0].item()]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Visualize patches\n",
    "print(\"Patches (16x16):\")\n",
    "visualize_patches(sample_image, patch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize Attention Maps\n",
    "\n",
    "Let's look at the attention patterns in ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to get attention maps\n",
    "def get_attention_maps(model, image):\n",
    "    # Add batch dimension\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass up to attention layer\n",
    "    attention_maps = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # Extract attention map from QK^T\n",
    "        q, k, v = input[0][0], input[0][1], input[0][2]  # B, H, N, C/H\n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attention_maps.append(attn.detach().cpu().numpy())\n",
    "    \n",
    "    # Register hooks for attention layers\n",
    "    hooks = []\n",
    "    for block in model.blocks:\n",
    "        hook = block.attn.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        model(image)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return attention_maps\n",
    "\n",
    "# Get attention maps for a sample image\n",
    "attention_maps = get_attention_maps(models[\"ViT-Tiny\"], sample_image)\n",
    "\n",
    "# Visualize attention maps\n",
    "def plot_attention_maps(attention_maps, layer_idx=0, head_idx=0, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Get attention map for specified layer and head\n",
    "    attn = attention_maps[layer_idx][0, head_idx]  # (N, N)\n",
    "    \n",
    "    # Display attention map\n",
    "    im = ax.imshow(attn, cmap='viridis')\n",
    "    ax.set_title(f\"Layer {layer_idx+1}, Head {head_idx+1}\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Add labels for CLS token and patches\n",
    "    ax.set_xlabel(\"Key (patches)\")\n",
    "    ax.set_ylabel(\"Query (patches)\")\n",
    "    ax.set_xticks([0] + list(range(5, attn.shape[1], 5)))\n",
    "    ax.set_yticks([0] + list(range(5, attn.shape[0], 5)))\n",
    "    ax.set_xticklabels([\"CLS\"] + [str(i) for i in range(5, attn.shape[1], 5)])\n",
    "    ax.set_yticklabels([\"CLS\"] + [str(i) for i in range(5, attn.shape[0], 5)])\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Plot attention maps for first layer, all heads\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i in range(3):  # ViT-Tiny has 3 heads\n",
    "    plot_attention_maps(attention_maps, layer_idx=0, head_idx=i, ax=axes[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot attention maps for all layers, first head\n",
    "n_layers = len(attention_maps)\n",
    "fig, axes = plt.subplots(1, n_layers, figsize=(4*n_layers, 4))\n",
    "for i in range(n_layers):\n",
    "    plot_attention_maps(attention_maps, layer_idx=i, head_idx=0, ax=axes[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore KAN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a ViT model with KAN layers\n",
    "vit_tiny_model = vit_tiny(num_classes=len(class_names))\n",
    "vit_with_kan = add_kan_layer(\n",
    "    vit_tiny_model, \n",
    "    kan_hidden_sizes=[128, 64], \n",
    "    num_classes=len(class_names),\n",
    "    kan_width=16\n",
    ")\n",
    "\n",
    "# Display model structure\n",
    "print(\"ViT with KAN Layers:\")\n",
    "print(vit_with_kan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize KAN layer structure\n",
    "kan_network = vit_with_kan.kan_classifier\n",
    "print(f\"KAN Network Structure:\\n{kan_network}\")\n",
    "\n",
    "# Show KAN layer parameters\n",
    "kan_layer = kan_network.layers[0]\n",
    "print(f\"\\nKAN Layer Parameters:\")\n",
    "print(f\"Input features: {kan_layer.in_features}\")\n",
    "print(f\"Output features: {kan_layer.out_features}\")\n",
    "print(f\"Width: {kan_layer.width}\")\n",
    "print(f\"W1 shape: {kan_layer.W1.shape}\")\n",
    "print(f\"W2 shape: {kan_layer.W2.shape}\")\n",
    "print(f\"Univariate weights shape: {kan_layer.univariate_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualize KAN Univariate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to visualize KAN univariate functions\n",
    "def visualize_kan_univariate_functions(kan_layer, n_samples=5):\n",
    "    # Generate x values\n",
    "    x = torch.linspace(-3, 3, 100)\n",
    "    \n",
    "    # Scale x for Chebyshev polynomials\n",
    "    x_scaled = x / 3.0\n",
    "    \n",
    "    # Generate Chebyshev polynomials\n",
    "    t0 = torch.ones_like(x_scaled)\n",
    "    t1 = x_scaled\n",
    "    t2 = 2 * x_scaled * t1 - t0\n",
    "    t3 = 2 * x_scaled * t2 - t1\n",
    "    t4 = 2 * x_scaled * t3 - t2\n",
    "    t5 = 2 * x_scaled * t4 - t3\n",
    "    t6 = 2 * x_scaled * t5 - t4\n",
    "    t7 = 2 * x_scaled * t6 - t5\n",
    "    \n",
    "    # Stack Chebyshev polynomials\n",
    "    cheb = torch.stack([t0, t1, t2, t3, t4, t5, t6, t7], dim=1)\n",
    "    \n",
    "    # Sample weights from univariate weights\n",
    "    weights = kan_layer.univariate_weights.detach().cpu()\n",
    "    bias = kan_layer.univariate_bias.detach().cpu()\n",
    "    \n",
    "    # Select a few random functions to visualize\n",
    "    indices = torch.randperm(weights.shape[0])[:n_samples]\n",
    "    \n",
    "    # Plot the functions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Compute function value\n",
    "        y = torch.matmul(cheb, weights[idx]) + bias[idx]\n",
    "        \n",
    "        # Plot\n",
    "        plt.subplot(n_samples, 1, i+1)\n",
    "        plt.plot(x.numpy(), y.numpy())\n",
    "        plt.grid(True)\n",
    "        plt.title(f\"Univariate Function {idx+1}\")\n",
    "        plt.xlabel(\"Input\")\n",
    "        plt.ylabel(\"Output\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize KAN univariate functions\n",
    "visualize_kan_univariate_functions(kan_layer, n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Models with Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test models with sample images\n",
    "def predict_image(model, image, class_names):\n",
    "    # Add batch dimension and move to device\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "    \n",
    "    # Get predicted class\n",
    "    pred_class = class_names[pred.item()]\n",
    "    pred_prob = probs[pred.item()].item()\n",
    "    \n",
    "    # Return top 3 predictions\n",
    "    top_probs, top_idxs = torch.topk(probs, 3)\n",
    "    top_classes = [class_names[idx.item()] for idx in top_idxs]\n",
    "    top_probs = top_probs.cpu().numpy()\n",
    "    \n",
    "    return pred_class, pred_prob, top_classes, top_probs\n",
    "\n",
    "# Get a few test images\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "n_images = 5\n",
    "\n",
    "# Initialize models\n",
    "vit_tiny_model = vit_tiny(num_classes=len(class_names)).to(device)\n",
    "vit_with_kan = add_kan_layer(vit_tiny(num_classes=len(class_names)), kan_hidden_sizes=[128, 64], \n",
    "                            num_classes=len(class_names), kan_width=16).to(device)\n",
    "\n",
    "# Test with random weights (not trained)\n",
    "plt.figure(figsize=(15, 4 * n_images))\n",
    "for i in range(n_images):\n",
    "    image = test_images[i]\n",
    "    true_class = class_names[test_labels[i].item()]\n",
    "    \n",
    "    # Make predictions with both models\n",
    "    pred_class1, pred_prob1, top_classes1, top_probs1 = predict_image(vit_tiny_model, image, class_names)\n",
    "    pred_class2, pred_prob2, top_classes2, top_probs2 = predict_image(vit_with_kan, image, class_names)\n",
    "    \n",
    "    # Display image\n",
    "    plt.subplot(n_images, 3, i*3 + 1)\n",
    "    plt.imshow(image.permute(1, 2, 0) / 2 + 0.5)\n",
    "    plt.title(f\"True Class: {true_class}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display predictions for ViT\n",
    "    plt.subplot(n_images, 3, i*3 + 2)\n",
    "    bars = plt.bar(range(3), top_probs1)\n",
    "    plt.xticks(range(3), top_classes1, rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(f\"ViT: {pred_class1} ({pred_prob1:.2f})\")\n",
    "    \n",
    "    # Display predictions for ViT+KAN\n",
    "    plt.subplot(n_images, 3, i*3 + 3)\n",
    "    bars = plt.bar(range(3), top_probs2)\n",
    "    plt.xticks(range(3), top_classes2, rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(f\"ViT+KAN: {pred_class2} ({pred_prob2:.2f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a ViT Model with KAN Layers\n",
    "\n",
    "To train the model, we would call the training script. From this notebook, we could set up a small training loop for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstration of a simple training loop (not full training)\n",
    "def train_for_demo(model, data_loader, num_batches=10):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "            \n",
    "        # Move to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        print(f\"Batch {i+1}/{num_batches}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train the KAN model for a few batches\n",
    "print(\"Training ViT with KAN for a few batches...\")\n",
    "losses = train_for_demo(vit_with_kan, train_loader, num_batches=5)\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this notebook, we have explored Vision Transformer models and KAN layers for the profession classification task. We have seen how to:\n",
    "\n",
    "1. Create and visualize ViT models of different sizes\n",
    "2. Understand the patch embedding and attention mechanisms\n",
    "3. Add KAN layers to ViT models\n",
    "4. Visualize the univariate functions in KAN layers\n",
    "5. Make predictions with ViT and ViT+KAN models\n",
    "\n",
    "For complete training, use the training scripts in the project:\n",
    "\n",
    "```bash\n",
    "python -m src.training.train --model vit_tiny --use_kan --optimizer adam --lr 0.0003 --batch_size 32 --augment\n",
    "```\n",
    "\n",
    "Or use hyperparameter optimization:\n",
    "\n",
    "```bash\n",
    "python -m src.training.optuna_optimization --data_dir data/idenprof --num_trials 20\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}